= Adding Applications and Authentication
:experimental:
:imagesdir: images

In this lab we'll deploy the applications that will consume Red Hat Single Sign-On for authentication.  The applications being deployed are:

- *Furever Home*: A Python and Static HTML Generator stack that provides Pet Adoption from a pool of available pets.  A microservice submits a new pet to a Kafka Topic which is then consumed by another service that inserts it into a database which is queried by a backend API service that feeds the frontend.  Once a user submits to adopt a pet there's another service that does a coin flip on if their adoption request is approved or not.
- *Pet ID*: A Golang and NodeJS (Vue) application stack that enhances the user's profile.

### 1. Deploy AMQ Streams

Before deploying the applications we need a Kafka cluster deployed - thankfully this is easy with https://www.redhat.com/en/technologies/jboss-middleware/amq[Red Hat AMQ Streams^].

[NOTE]
====
Navigate back to your {{ ECLIPSE_CHE_URL }}[CodeReady Workspaces Terminal^] if you're not already in it.
====

Make sure you're in the right Project:

[source,sh,role="copypaste"]
----
oc project {{ USER_ID}}-sso
----

Deploy a Kafka Cluster with the AMQ Streams Operator:

[source,sh,role="copypaste"]
----
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/python-staticjs-furever-home/python-add-adoptee-usvc/openshift/kafka-instance/01-kafka-cluster.yaml
----

Go back to the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-sso[Topology View^] and watch the Kafka cluster deploy automatically:

image::rhsso_finished_kafka_cluster.png[sso, 1100]

### 2. Create Kafka Topics

Next we'll deploy our Kafka topics that will be relaying the messages back and forth from these different application services.

[NOTE]
====
Navigate back to your {{ ECLIPSE_CHE_URL }}[CodeReady Workspaces Terminal^] if you're not already in it.
====

Deploy a Kafka Topic:

[source,sh,role="copypaste"]
----
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/python-staticjs-furever-home/python-add-adoptee-usvc/openshift/kafka-topic/01-kafka-topic.yaml
----

There is nothing shown in the Topology UI that will show the Kafka Topic outside of exploring instances of the CRD.

### 3. Deploy Database Services

All good applications use databases and this set is no different - we'll deploy a few databases simply on OpenShift.

[NOTE]
====
Navigate back to your {{ ECLIPSE_CHE_URL }}[CodeReady Workspaces Terminal^] if you're not already in it.
====

Deploy the databases needed for our services:

[source,sh,role="copypaste"]
----
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/python-staticjs-furever-home/python-process-adoptee-usvc/openshift/database/01-template-instance.yaml && \
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/go-nodejs-pet-id/go-usvc/openshift/step1/00-db-template-instance.yaml
----

Go back to the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-sso[Topology View^] and watch the databases deploy:

image::rhsso_databases_deployed.png[sso, 1100]

Now that the databases are deployed, navigate back to the {{ ECLIPSE_CHE_URL }}[CodeReady Workspaces Terminal^] and run the Database Migration Jobs:

[source,sh,role="copypaste"]
----
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/python-staticjs-furever-home/python-process-adoptee-usvc/openshift/database-config/db-schema-migration-job.yaml && \
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/go-nodejs-pet-id/go-usvc/openshift/step2/db-schema-migration-job.yaml
----

If you navigate back to the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-sso[Topology View^] you can find the completed Jobs that added the database schema:

image::rhsso_database_jobs_deployed.png[sso, 1100]

### 4. Deploy Applications

With all the dependent services deployed we can now provision our actual applications.

[NOTE]
====
Navigate back to your {{ ECLIPSE_CHE_URL }}[CodeReady Workspaces Terminal^] if you're not already in it.
====

Deploy the *Furever Home Backend* Python application:

[source,sh,role="copypaste"]
----
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/python-staticjs-furever-home/python-backend/openshift/
----

Deploy the *Furever Home Add Adoptee* Python service:

[source,sh,role="copypaste"]
----
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/python-staticjs-furever-home/python-add-adoptee-usvc/openshift/add-adoptee-usvc/01-deployment.yaml
----

Deploy the *Furever Home Process Adoptee* Python service:

[source,sh,role="copypaste"]
----
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/python-staticjs-furever-home/python-process-adoptee-usvc/openshift/process-adoptee-usvc/01-deployment.yaml
----

Deploy the *Furever Home Adoption Judge* Python service:

[source,sh,role="copypaste"]
----
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/python-staticjs-furever-home/python-adoption-judge-usvc/openshift/
----

At this point, the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-sso[Topology View^] of the {{ USER_ID }}-sso project should look something like this:

image::rhsso_basic_app_topology.png[sso, 885]

Those were the easy to deploy applications where the manifests are already composed - next let's deploy the *Furever Home Frontend* which will need a ConfigMap to set parameters for the application to consume.

First, take the RH SSO Route and access it as an environmental variable:

[source,sh,role="copypaste"]
----
RH_SSO_ROUTE=$(oc get route secure-sso -n {{ USER_ID}}-sso -o jsonpath='{.status.ingress[0].host}')
APPS_ROUTE_BASE=$(oc get route secure-sso -n {{ USER_ID}}-sso -o jsonpath='{.status.ingress[0].routerCanonicalHostname}')
----

Next, template out the ConfigMap and pipe into the `oc` CLI:

[source,sh,role="copypaste"]
----
cat <<EOF | oc apply -n {{ USER_ID}}-sso -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: furever-home-frontend-js-overrides
  labels:
    app: furever-home-frontend
    app.kubernetes.io/name: furever-home-frontend
    app.kubernetes.io/part-of: furever-home-frontend
    component: configmap
data:
  overrides.js: |
    var API_ENDPOINT = "https://furever-home-backend-{{ USER_ID}}-sso.${APPS_ROUTE_BASE}";
    var JUDGE_ENDPOINT = "https://furever-home-adoption-judge-usvc-{{ USER_ID}}-sso.${APPS_ROUTE_BASE}";
    var PROFILES_ENDPOINT = "https://pet-id-backend-{{ USER_ID}}-sso.${APPS_ROUTE_BASE}/app"
    var USER_ID = 1;
  functions.js: |
    console.log('hello from the functions file!');
  keycloak-shared.js: |
    function initKeycloak() {
      const KeycloakServer = "https://${RH_SSO_ROUTE}/auth/";
      const KeycloakRealm = "petcorp";
      const KeycloakClientID = "furever-home";
      let initOptions = {
        url: KeycloakServer, realm: KeycloakRealm, clientId: KeycloakClientID, onLoad: 'login-required'
      }
      var keycloak = new Keycloak(initOptions);
      keycloak.init({ onLoad: initOptions.onLoad }).then(function(authenticated) {
        if (authenticated) {
          username = keycloak.idTokenParsed.preferred_username;
          token = keycloak.token;
          USER_ID = keycloak.subject;
          jQuery(".text-username").text(username);
          // Query the Profiles API for the Avatar
          jQuery.ajax(PROFILES_ENDPOINT + '/profile?user_id=' + USER_ID, {
            success: function (data, status, xhr) {
              jsonO = JSON.parse(data);
              jsonI = jsonO.entities[0].profiles[0]
              jQuery("h1.h4 a.float-right").append('<img src="'+ jsonI.avatar_url +'" alt="Avatar" style="max-width: 60px;max-height: 60px;border-radius: 50%;margin: 0 0 0 1rem;" />');
            }
          });
          loadSubmissions();
        }
      }).catch(function(e) {
        console.log('failed to initialize');
        console.log(e);
      });
    }
  keycloak.json: |
    {
      "realm": "petcorp",
      "auth-server-url": "https://${RH_SSO_ROUTE}/auth/",
      "ssl-required": "none",
      "resource": "furever-home",
      "public-client": true,
      "confidential-port": 0
    }
EOF
----

Now deploy the actual application:

[source,sh,role="copypaste"]
----
oc apply -n {{ USER_ID}}-sso  -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/python-staticjs-furever-home/static-frontend/openshift/01-deployment.yaml && \
oc apply -n {{ USER_ID}}-sso  -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/python-staticjs-furever-home/static-frontend/openshift/02-service.yaml && \
oc apply -n {{ USER_ID}}-sso  -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/python-staticjs-furever-home/static-frontend/openshift/03-route.yaml
----

The {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-sso[Topology View^] of the {{ USER_ID }}-sso project should look something like this:

image::rhsso_fh_fe_app_topology.png[sso, 885]

There are two more applications that need to be deployed - next is the *PetID Backend* which has a templated Deployment.

First, apply the supporting PetID Backend manifests:

[source,sh,role="copypaste"]
----
oc apply -n {{ USER_ID}}-sso  -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/go-nodejs-pet-id/go-usvc/openshift/step1/01-configmap.yaml && \
oc apply -n {{ USER_ID}}-sso  -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/go-nodejs-pet-id/go-usvc/openshift/step1/02-service.yaml && \
oc apply -n {{ USER_ID}}-sso  -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/go-nodejs-pet-id/go-usvc/openshift/step1/04-route.yaml
----

Next, deploy the application - the templated components are with the environmental variables set on the container.

[source,sh,role="copypaste"]
----
cat <<EOF | oc apply -n {{ USER_ID}}-sso -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pet-id-backend
  labels:
    app: pet-id-backend
    app.kubernetes.io/name: pet-id-backend
    app.kubernetes.io/part-of: pet-id
    component: deployment
  annotations:
    app.openshift.io/connects-to: >-
      [{"apiVersion":"apps.openshift.io/v1","kind":"DeploymentConfig","name":"petid-db"}]
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pet-id-backend
      component: deployment
  template:
    metadata:
      labels:
        app: pet-id-backend
        component: deployment
    spec:
      containers:
      - name: pet-id-backend
        imagePullPolicy: Always
        image: quay.io/kenmoini/pet-id-backend:latest
        ports:
        - containerPort: 8080
          name: pet-id-backend
        env:
          - name: KEYCLOAK_SERVER
            value: http://sso:8080/auth
          - name: KEYCLOAK_REALM
            value: petcorp
          - name: KEYCLOAK_CLIENT_ID
            value: pet-id
          - name: THREE_SCALE_SECRET_HEADER
            value: someSecretHeaderKeyValue
        volumeMounts:
          - name: config
            mountPath: "/opt/app-root/"
            readOnly: true
      volumes:
        - name: config
          configMap:
            name: pet-id-backend-config-yml
            items:
              - key: "config.yml"
                path: "config.yml"
EOF
----

With the PetID Backend deployed the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-sso[Topology View^] of the {{ USER_ID }}-sso project should look something like this:

image::rhsso_pid_be_app_topology.png[sso, 885]

Our final application is the *PetID Frontend* - this application takes some minor modifications to deploy and it will be built live on OpenShift!

Before deploying, we need to modify a file quickly and to do so we need a few variables:

[source,sh,role="copypaste"]
----
RH_SSO_ROUTE=$(oc get route secure-sso -n {{ USER_ID}}-sso -o jsonpath='{.status.ingress[0].host}')
PET_ID_ROUTE=$(oc get route pet-id-backend -n {{ USER_ID}}-sso -o jsonpath='{.status.ingress[0].host}')

echo -e "\nRH_SSO_ROUTE: ${RH_SSO_ROUTE}\nPET_ID_BACKEND_ROUTE: ${PET_ID_ROUTE}\n"
----

[NOTE]
====
You will need those URLs to modify the needed JavaScript file
====

Next, in your {{ ECLIPSE_CHE_URL }}[CodeReady Workspaces Editor^] open the file `services/go-nodejs-pet-id/app-vue/src/vars.js`

You will see two parts where you should edit - substitute the corresponding URLs from the terminal into the file in its respective places at `RH_SSO_ROUTE` and `PET_ID_BACKEND_ROUTE`

image::rhsso_app_vars_edit.png[sso, 1190]

[NOTE]
====
There is no need to save the file - it is automatically saved
====

Now with our application configured, we can deploy to OpenShift from the source code we have locally in CodeReady Workspaces.

Define a new Application that will use the Docker build strategy and take binary input for the source:

[source,sh,role="copypaste"]
----
oc new-app -n {{ USER_ID}}-sso --strategy=docker --name pet-id-frontend --binary
----

Next we'll start a new Build of that Application:

[source,sh,role="copypaste"]
----
oc start-build pet-id-frontend -n {{ USER_ID}}-sso --from-dir="$CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/go-nodejs-pet-id/app-vue/"
----

Finally, add the Service and Route to the Application Deployment:

[source,sh,role="copypaste"]
----
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/go-nodejs-pet-id/app-vue/openshift/02-service.yaml && \
oc apply -n {{ USER_ID}}-sso -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m5-labs/services/go-nodejs-pet-id/app-vue/openshift/03-route.yaml
----

You can watch the build in the {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-sso[Topology View^] of the {{ USER_ID }}-sso project:

INSERT BUILD IMAGE HERE

### 5. Explore Applications
